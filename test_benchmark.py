#!/usr/bin/env python3
"""
Test script for HuggingFace benchmarking framework
Checks if all components are working properly
"""

import os
import sys

def test_imports():
    """Test if all required modules can be imported"""
    print("üîç Testing imports...")
    
    test_modules = [
        ("model_configs", "Model configurations"),
        ("benchmark_utils", "Benchmark utilities"),  
        ("data_loader", "Data loader"),
        ("huggingface_benchmark", "Main benchmark script"),
        ("results_analyzer", "Results analyzer"),
    ]
    
    success_count = 0
    
    for module, description in test_modules:
        try:
            __import__(module)
            print(f"‚úì {description}")
            success_count += 1
        except ImportError as e:
            print(f"‚úó {description}: {e}")
    
    print(f"\nImport test: {success_count}/{len(test_modules)} successful")
    return success_count == len(test_modules)

def test_model_configs():
    """Test model configurations"""
    print("\nüéØ Testing model configurations...")
    
    try:
        from model_configs import BENCHMARK_MODELS, get_model_config, MODEL_SETS
        
        print(f"‚úì Found {len(BENCHMARK_MODELS)} models configured")
        print(f"‚úì Found {len(MODEL_SETS)} model sets configured")
        
        # Test a specific model
        test_model = get_model_config("efficientnet_b0")
        print(f"‚úì Test model: {test_model.name} ({test_model.parameters:,} parameters)")
        
        return True
    except Exception as e:
        print(f"‚úó Model config test failed: {e}")
        return False

def test_data_loader():
    """Test data loading functionality"""
    print("\nüìä Testing data loader...")
    
    try:
        from data_loader import COINS, find_candlestick_data, list_available_data
        
        print(f"‚úì Found {len(COINS)} coins configured")
        
        # Test data discovery
        print("‚úì Testing data discovery...")
        
        # Look for any available data
        data_found = False
        for coin in ["BTCUSDT", "ETHUSDT"]:
            for period in ["7days", "14days"]:
                for window_size in [5, 15]:
                    for exp_type in ["regular", "fullimage"]:
                        try:
                            data_files = find_candlestick_data(coin, period, window_size, exp_type)
                            if data_files:
                                print(f"‚úì Found data: {coin} - {period} - w{window_size} - {exp_type} ({len(data_files)} files)")
                                data_found = True
                                break
                        except:
                            continue
                    if data_found:
                        break
                if data_found:
                    break
            if data_found:
                break
        
        if not data_found:
            print("‚ö†Ô∏è  No candlestick data found. You may need to generate data first:")
            print("   python merged_candlestick.py --experiment regular")
        
        return True
        
    except Exception as e:
        print(f"‚úó Data loader test failed: {e}")
        return False

def test_benchmark_utils():
    """Test benchmark utilities"""
    print("\nüõ†Ô∏è  Testing benchmark utilities...")
    
    try:
        from benchmark_utils import get_device_info, ModelAdapter
        
        # Test device info
        device_info = get_device_info()
        print(f"‚úì Device: {device_info.get('device_type', 'unknown')}")
        
        # Test model adapter (without actually loading a model)
        print("‚úì Model adapter class available")
        
        return True
        
    except Exception as e:
        print(f"‚úó Benchmark utils test failed: {e}")
        return False

def test_directory_structure():
    """Test directory structure"""
    print("\nüìÅ Testing directory structure...")
    
    required_dirs = ["benchmarks", "benchmarks/results", "benchmarks/models", "benchmarks/reports"]
    
    for dir_path in required_dirs:
        if os.path.exists(dir_path):
            print(f"‚úì {dir_path}")
        else:
            print(f"! Creating {dir_path}")
            os.makedirs(dir_path, exist_ok=True)
    
    return True

def main():
    print("üöÄ HuggingFace Benchmarking Framework Test Suite")
    print("=" * 60)
    
    tests = [
        ("Imports", test_imports),
        ("Model Configurations", test_model_configs),
        ("Data Loader", test_data_loader),
        ("Benchmark Utils", test_benchmark_utils),
        ("Directory Structure", test_directory_structure),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"‚úó {test_name} test crashed: {e}")
    
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)
    print(f"Passed: {passed}/{total} tests")
    
    if passed == total:
        print("üéâ All tests passed! The framework is ready to use.")
        print("\nNext steps:")
        print("1. Generate candlestick data (if not done already):")
        print("   python merged_candlestick.py --experiment regular")
        print("\n2. Run a quick benchmark:")
        print("   python huggingface_benchmark.py --model-set quick_test --max-samples 200 --epochs 2")
        print("\n3. List available data:")
        print("   python huggingface_benchmark.py --list-data")
    else:
        print(f"‚ö†Ô∏è  {total - passed} tests failed. Please fix the issues before running benchmarks.")
        print("\nCommon fixes:")
        print("- Install requirements: pip install -r requirements_benchmark.txt")
        print("- Generate data: python merged_candlestick.py --experiment regular")
    
    print("=" * 60)

if __name__ == "__main__":
    main()